<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>body {
  max-width: 980px;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable,
.markdown-body .highlighttable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr,
.markdown-body .highlighttable {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite,
.markdown-body .highlighttable pre,
.markdown-body .highlighttable div.highlight {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td,
.markdown-body .highlighttable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  border-radius: 3px;
}

.markdown-body code:not(.highlight):not(.codehilite), .markdown-body samp {
  background-color: rgba(0,0,0,0.04);
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite,
.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
}

.markdown-body .codehilite,
.markdown-body .highlight,
.markdown-body pre {
  border-radius: 3px;
}

.markdown-body :not(.highlight) > pre {
  background-color: #f7f7f7;
}

.markdown-body .codehilite pre,
.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

.markdown-body diagram-div, .markdown-body div.uml-sequence-diagram, .markdown-body, div.uml-flowchart {
  overflow: auto;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*GitHub*/
.highlight {background-color:#f7f7f7;color:#333333;}
.highlight .hll {background-color:#ffffcc;}
.highlight .c{color:#999988;font-style:italic}
.highlight .err{color:#a61717;background-color:#e3d2d2}
.highlight .k{font-weight:bold}
.highlight .o{font-weight:bold}
.highlight .cm{color:#999988;font-style:italic}
.highlight .cp{color:#999999;font-weight:bold}
.highlight .c1{color:#999988;font-style:italic}
.highlight .cs{color:#999999;font-weight:bold;font-style:italic}
.highlight .gd{color:#000000;background-color:#ffdddd}
.highlight .ge{font-style:italic}
.highlight .gr{color:#aa0000}
.highlight .gh{color:#999999}
.highlight .gi{color:#000000;background-color:#ddffdd}
.highlight .go{color:#888888}
.highlight .gp{color:#555555}
.highlight .gs{font-weight:bold}
.highlight .gu{color:#800080;font-weight:bold}
.highlight .gt{color:#aa0000}
.highlight .kc{font-weight:bold}
.highlight .kd{font-weight:bold}
.highlight .kn{font-weight:bold}
.highlight .kp{font-weight:bold}
.highlight .kr{font-weight:bold}
.highlight .kt{color:#445588;font-weight:bold}
.highlight .m{color:#009999}
.highlight .s{color:#dd1144}
.highlight .n{color:#333333}
.highlight .na{color:teal}
.highlight .nb{color:#0086b3}
.highlight .nc{color:#445588;font-weight:bold}
.highlight .no{color:teal}
.highlight .ni{color:purple}
.highlight .ne{color:#990000;font-weight:bold}
.highlight .nf{color:#990000;font-weight:bold}
.highlight .nn{color:#555555}
.highlight .nt{color:navy}
.highlight .nv{color:teal}
.highlight .ow{font-weight:bold}
.highlight .w{color:#bbbbbb}
.highlight .mf{color:#009999}
.highlight .mh{color:#009999}
.highlight .mi{color:#009999}
.highlight .mo{color:#009999}
.highlight .sb{color:#dd1144}
.highlight .sc{color:#dd1144}
.highlight .sd{color:#dd1144}
.highlight .s2{color:#dd1144}
.highlight .se{color:#dd1144}
.highlight .sh{color:#dd1144}
.highlight .si{color:#dd1144}
.highlight .sx{color:#dd1144}
.highlight .sr{color:#009926}
.highlight .s1{color:#dd1144}
.highlight .ss{color:#990073}
.highlight .bp{color:#999999}
.highlight .vc{color:teal}
.highlight .vg{color:teal}
.highlight .vi{color:teal}
.highlight .il{color:#009999}
.highlight .gc{color:#999;background-color:#EAF2F5}
</style><title>readme</title></head><body><article class="markdown-body"><h2 id="ml-pipeline">ML pipeline:<a class="headerlink" href="#ml-pipeline" title="Permanent link"></a></h2>
<ol>
<li>import data</li>
<li>clean data, preprocessing. For example: handle missing data(throw away, replace&hellip;)</li>
<li>normalize data</li>
<li>learm machine. For example: create neural network and feed training data set</li>
<li>tune(regularization)</li>
<li>test result with control data set</li>
</ol>
<h2 id="tensor">Tensor<a class="headerlink" href="#tensor" title="Permanent link"></a></h2>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/tensor.png"></p>
<p>Tensors are the fundamental building block of machine learning.</p>
<p>Their job is to represent data in a numerical way.</p>
<p>In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors.</p>
<p>Is a multi-dimensional matrix containing elements.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/datatypes_tensor.jpg"></p>
<h3 id="image-representation-example">Image representation example<a class="headerlink" href="#image-representation-example" title="Permanent link"></a></h3>
<p>Its possible to represent an image as a tensor with shape <code>[3, 224, 224]</code> which would mean <code>[colour_channels, height, width]</code>, as in the image has 3 colour channels (red, green, blue), a height of 224 pixels and a width of 224 pixels. In tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for colour_channels, height and width.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/image_tensor_representation.png"></p>
<p><a href="https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors">https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors</a></p>
<p><a href="https://en.wikipedia.org/wiki/Tensor">https://en.wikipedia.org/wiki/Tensor</a></p>
<h2 id="train-model">Train model<a class="headerlink" href="#train-model" title="Permanent link"></a></h2>
<p>The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process. </p>
<p>The learning algorithm finds patterns in the training data that map the input data attributes to the target (the answer that you want to predict), and it outputs an ML model that captures these patterns. </p>
<h3 id="hyperparameter">Hyperparameter<a class="headerlink" href="#hyperparameter" title="Permanent link"></a></h3>
<p>In machine learning, a hyperparameter is a parameter, such as the learning rate or choice of optimizer, which specifies details of the learning process, hence the name hyperparameter. This is in contrast to parameters which determine the model itself. </p>
<p><a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning">https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning</a>)</p>
<h3 id="loss-function">Loss function<a class="headerlink" href="#loss-function" title="Permanent link"></a></h3>
<p>At its core, a loss function is incredibly simple: It&rsquo;s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they&rsquo;re pretty good, it&rsquo;ll output a lower number.</p>
<p><strong>Interpreting Loss Curves</strong>: <a href="https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic">https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/04-loss-curves-overfitting-underfitting-ideal.jpg"></p>
<p><a href="https://en.wikipedia.org/wiki/Loss_function">https://en.wikipedia.org/wiki/Loss_function</a></p>
<h3 id="traintest-split-procedure">Train/Test Split Procedure<a class="headerlink" href="#traintest-split-procedure" title="Permanent link"></a></h3>
<ol>
<li>Clean and adjust data as necessary for X and y</li>
<li>Split Data in Train/Test for both X and y</li>
<li>Fit/Train Scaler on Training X Data</li>
<li>Scale X Test Data</li>
<li>Create Model</li>
<li>Fit/Train Model on X Train Data</li>
<li>Evaluate Model on X Test Data (by creating predictions and comparing to Y_test)</li>
<li>Adjust Parameters as Necessary and repeat steps 6 and 7</li>
</ol>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;../DATA/Advertising.csv&quot;</span><span class="p">)</span>

<span class="c1">## 1. CREATE X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;sales&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;sales&#39;</span><span class="p">]</span>

<span class="c1"># 2. TRAIN TEST SPLIT</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># 3. TRAIN SCALER</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># 4 SCALE DATA</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="c1"># 5. Poor Alpha Choice on purpose!</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># 6. Fit/Train Model on X Train Data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 7. evaluation</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span> <span class="c1"># 7.34177578903413</span>

<span class="c1"># 8. Adjust Parameters and Re-evaluate</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span> <span class="c1"># 2.319021579428752</span>
</pre></div>

<h3 id="trainvalidationtest-split-procedure">Train/Validation/Test Split Procedure<a class="headerlink" href="#trainvalidationtest-split-procedure" title="Permanent link"></a></h3>
<p>This is often also called a &ldquo;hold-out&rdquo; set, since you should not adjust parameters based on the final test set, but instead use it only for reporting final expected performance.</p>
<ol>
<li>Clean and adjust data as necessary for X and y</li>
<li>Split Data in Train/Validation/Test for both X and y</li>
<li>Fit/Train Scaler on Training X Data</li>
<li>Scale X Eval Data</li>
<li>Create Model</li>
<li>Fit/Train Model on X Train Data</li>
<li>Evaluate Model on X Evaluation Data (by creating predictions and comparing to Y_eval)</li>
<li>Adjust Parameters as Necessary and repeat steps 6 and 7</li>
<li>Get final metrics on Test set (not allowed to go back and adjust after this!)</li>
</ol>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;../DATA/Advertising.csv&quot;</span><span class="p">)</span>

<span class="c1">## 1. CREATE X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;sales&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;sales&#39;</span><span class="p">]</span>

<span class="c1">######################################################################</span>
<span class="c1">#### SPLIT TWICE! Here we create TRAIN | VALIDATION | TEST  #########</span>
<span class="c1">####################################################################</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># 70% of data is training data, set aside other 30%</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_OTHER</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_OTHER</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Remaining 30% is split into evaluation and test sets</span>
<span class="c1"># Each is 15% of the original data size</span>
<span class="n">X_eval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_eval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_OTHER</span><span class="p">,</span> <span class="n">y_OTHER</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># SCALE DATA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_eval</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_eval</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_eval_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_eval</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Evaluation</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_eval</span><span class="p">,</span><span class="n">y_eval_pred</span><span class="p">)</span> <span class="c1"># 7.320101458823871</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_eval_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_eval</span><span class="p">)</span>

<span class="c1"># Another Evaluation</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_eval</span><span class="p">,</span><span class="n">y_eval_pred</span><span class="p">)</span> <span class="c1"># 2.383783075056986</span>

<span class="c1"># Final Evaluation (Can no longer edit parameters after this!)</span>
<span class="n">y_final_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_final_test_pred</span><span class="p">)</span> <span class="c1"># 2.254260083800517</span>
</pre></div>

<h3 id="cross-validation">Cross validation<a class="headerlink" href="#cross-validation" title="Permanent link"></a></h3>
<p>Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.</p>
<p><img src="images/grid_search_workflow.png" height=350></p>
<p>When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</p>
<p>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</p>
<p>A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:</p>
<ul>
<li>
<p>A model is trained using <code>k - 1</code>of the folds as training data;</p>
</li>
<li>
<p>the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).</p>
</li>
</ul>
<p>The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</p>
<p><img src="images/grid_search_cross_validation.png" height=350></p>
<div class="highlight"><pre><span class="c1">## CREATE X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;sales&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;sales&#39;</span><span class="p">]</span>

<span class="c1"># TRAIN TEST SPLIT</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># SCALE DATA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="c1"># SCORING OPTIONS:</span>
<span class="c1"># https://scikit-learn.org/stable/modules/model_evaluation.html</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
                         <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;neg_mean_absolute_error&#39;</span><span class="p">,</span><span class="s">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span><span class="s">&#39;max_error&#39;</span><span class="p">],</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>


<span class="c1"># Adjust model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># SCORING OPTIONS:</span>
<span class="c1"># https://scikit-learn.org/stable/modules/model_evaluation.html</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
                         <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;neg_mean_absolute_error&#39;</span><span class="p">,</span><span class="s">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span><span class="s">&#39;max_error&#39;</span><span class="p">],</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>


<span class="c1"># Final Evaluation (Can no longer edit parameters after this!)</span>
<span class="c1"># Need to fit the model first!</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_final_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_final_test_pred</span><span class="p">)</span> <span class="c1"># 2.319021579428752</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></p>
<p><a href="https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/">scikit-learn course, Section 11: Feature Engineering and Data Preparation</a></p>
<h3 id="grid-search">Grid Search<a class="headerlink" href="#grid-search" title="Permanent link"></a></h3>
<p>We can search through a variety of combinations of hyperparameters with a grid search. While many linear models are quite simple and even come with their own specialized versions that do a search for you, this method of a grid search will can be applied to any model from sklearn.</p>
<p>A search consists of:</p>
<ul>
<li>an estimator (regressor or classifier such as sklearn.svm.SVC());</li>
<li>a parameter space;</li>
<li>a method for searching or sampling candidates;</li>
<li>a cross-validation scheme</li>
<li>a score function.</li>
</ul>
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;../DATA/Advertising.csv&quot;</span><span class="p">)</span>
<span class="c1">## CREATE X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;sales&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;sales&#39;</span><span class="p">]</span>

<span class="c1"># TRAIN TEST SPLIT</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># SCALE DATA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">base_elastic_model</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">()</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;alpha&#39;</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span>
              <span class="s">&#39;l1_ratio&#39;</span><span class="p">:[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># verbose number a personal preference</span>
<span class="n">grid_model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">base_elastic_model</span><span class="p">,</span>
                          <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                          <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
                          <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="c1"># ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=1,</span>
<span class="c1">#            max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="c1">#            random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># {&#39;alpha&#39;: 0.1, &#39;l1_ratio&#39;: 1}</span>


<span class="c1"># Using Best Model From Grid Search</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">grid_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span> <span class="c1"># 2.3873426420874737</span>
</pre></div>

<h2 id="algorithms">Algorithms<a class="headerlink" href="#algorithms" title="Permanent link"></a></h2>
<h3 id="classification-vs-regression">Classification vs Regression<a class="headerlink" href="#classification-vs-regression" title="Permanent link"></a></h3>
<p><strong>Regression task</strong> - continious value to predict (example: future prices, electricity loads, etc)</p>
<p><strong>Classification task</strong> - categorical value to predict</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/regression_vs_classification.jpg"></p>
<h4 id="binary-classification">Binary classification<a class="headerlink" href="#binary-classification" title="Permanent link"></a></h4>
<p>is the task of classifying the elements of a set into one of two groups (each called class).
Some of the methods commonly used for binary classification are:</p>
<ul>
<li>Decision trees</li>
<li>Random forests</li>
<li>Bayesian networks</li>
<li>Support vector machines</li>
<li>Neural networks</li>
<li>Logistic regression</li>
<li>Probit model</li>
<li>Genetic Programming</li>
<li>Multi expression programming</li>
<li>Linear genetic programming </li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Binary_classification">https://en.wikipedia.org/wiki/Binary_classification</a></p>
<h4 id="multiclass-classification">Multiclass classification<a class="headerlink" href="#multiclass-classification" title="Permanent link"></a></h4>
<p>is the problem of classifying instances into one of three or more classes</p>
<p>Several algorithms have been developed based on: </p>
<ul>
<li>neural networks</li>
<li>decision trees</li>
<li>k-nearest neighbors</li>
<li>naive Bayes</li>
<li>support vector machines </li>
<li>extreme learning machines </li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Multiclass_classification">https://en.wikipedia.org/wiki/Multiclass_classification</a></p>
<h4 id="multi-label-classification">Multi-label classification<a class="headerlink" href="#multi-label-classification" title="Permanent link"></a></h4>
<p>Target can be assigned more than one option</p>
<h3 id="deep-learning-vs-shallow-learning">Deep learning vs shallow learning<a class="headerlink" href="#deep-learning-vs-shallow-learning" title="Permanent link"></a></h3>
<p>Shallow learning (algorithms from scikit) for structured data (columns, tabels, etc)</p>
<p>Deep learning (NN) is better for unstructured data (video, audio, etc)</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/deep_vs_shallow.jpg"></p>
<h3 id="supervised-learning">Supervised learning<a class="headerlink" href="#supervised-learning" title="Permanent link"></a></h3>
<p>is a category of machine learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns.</p>
<h4 id="linear-regression">Linear Regression<a class="headerlink" href="#linear-regression" title="Permanent link"></a></h4>
<p>[regression]</p>
<p>is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable&rsquo;s value is called the independent variable.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/linear_regression.jpg"></p>
<h5 id="metrics">Metrics<a class="headerlink" href="#metrics" title="Permanent link"></a></h5>
<p>Mean Absolute Error (MAE) is the mean of the absolute value of the errors:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/mean_absolute_error.jpg"></p>
<p>Mean Squared Error (MSE) is the mean of the squared errors:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/mean_squared_error.jpg"></p>
<p>Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/root_mean_squared_error.jpg"></p>
<p>Comparing these metrics:</p>
<ul>
<li><strong>MAE</strong> is the easiest to understand, because it&rsquo;s the average error.</li>
<li><strong>MSE</strong> is more popular than MAE, because MSE &ldquo;punishes&rdquo; larger errors, which tends to be useful in the real world.</li>
<li><strong>RMSE</strong> is even more popular than MSE, because RMSE is interpretable in the &ldquo;y&rdquo; units.</li>
<li><strong>RSE</strong></li>
</ul>
<p>All of these are loss functions, because we want to minimize them.</p>
<p><strong>Residual</strong> is the difference between the actual value and the value predicted by the model (y-ŷ) for any given point.</p>
<p>A kernel <strong>density</strong> estimate plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/residual_0.jpg" width="290">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/residual_1.jpg" width="290">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/residual_2.jpg" width="290"></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/lr_residual_invalid.jpg"></p>
<p>Acceptable values of metrics depend on context, so meaning of numbers is relative here.</p>
<p><strong>Scikit-learn example:</strong>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> <span class="c1"># creating a model instance</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># training a model</span>

<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># it returns predicted value in array</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span>

<span class="c1"># computing of model performance metrics</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span> <span class="c1">#import numpy as np</span>
</pre></div></p>
<h4 id="polynomial-regression">Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permanent link"></a></h4>
<p>[regression]</p>
<p>is a form of regression analysis in which higher-degree functions of the independent variable, such as squares and cubes, are used to fit the data. It allows for more complex relationships between variables compared to linear regression.</p>
<p>It can help when:</p>
<ul>
<li>signal from existing feature is to weak, example:</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/polynominal_log.jpg" height="330"></p>
<ul>
<li>when relationship between the feature(s) and the response variable can’t be best described with a straight line</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/polynomial-regression-3.png" height="370"></p>
<ul>
<li>when two features have meaning only when used together in sync (synergy). Interaction between all pairs of features. In other words permutations of features can show ussefull signal </li>
</ul>
<p>Creates more features from the original x feature for some d degree of polynomial.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/polynominial.jpg"></p>
<p>Then we can call the linear regression model on it, since in reality, we&rsquo;re just treating these new polynomial features <code>x^2, x^3, ... x^d</code> as new features. Obviously we need to be careful about choosing the correct value of <strong>d</strong> , the degree of the model. </p>
<p>The other thing to note here is we have multiple X features, not just a single one as in the formula above, so in reality, the PolynomialFeatures will also take interaction terms into account for example, if an input sample is two dimensional and of the form <code>[a, b]</code>, the degree-2 polynomial features are <code>[1, a, b, a^2, ab, b^2]</code>.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">polinominal_converter</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">poly_features</span> <span class="o">=</span> <span class="n">polinominal_converter</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">poly_features</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># result. it returns predicted value in array</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="c1"># computing of model performance metrics</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span> <span class="c1">#import numpy as np</span>

<span class="c1">### **** note ****</span>
<span class="c1"># model.predict() expects output of polinominal_converter.fit_transform(()</span>
<span class="c1"># For example if new data came in stack of calls would look like:</span>
<span class="n">new_data_item</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">149</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">12</span><span class="p">]]</span>
<span class="n">new_data_item_featres</span> <span class="o">=</span> <span class="n">polinominal_converter</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_data_item</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_data_item_featres</span><span class="p">)</span>
</pre></div>

<h4 id="ridge-regression">Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permanent link"></a></h4>
<p>[regression]</p>
<p>is a statistical regularization technique. It corrects for overfitting on training data in machine learning models. Ridge regression—also known as L2 regularization—is one of several types of regularization for linear regression models.</p>
<p>In machine learning, ridge regression helps reduce overfitting that results from model complexity. Model complexity can be due to:</p>
<ul>
<li>
<p>A model possessing too many features. Features are the model’s predictors and may also be called “parameters” in machine learning. Online tutorials often recommend keeping the number of features below the number of instances in training data sets. Such is not always be feasible however.</p>
</li>
<li>
<p>Features possessing too much weight. Feature weight refers to a given predictor’s effect on the model output. A high feature weight is equivalent to a high-value coefficient.</p>
</li>
</ul>
<p>Simpler models do not intrinsically perform better then complex models. Nevertheless, a high degree of model complexity can inhibit a model’s ability to generalize on new data outside of the training set.</p>
<p>Because ridge regression does not perform feature selection, it cannot reduce model complexity by eliminating features. But if one or more features too heavily affect a model’s output, ridge regression ridge regression can shrink high feature weights (i.e. coefficients) across the model per the L2 penalty term. This reduces the complexity of the model and helps make model predictions less erratically dependent on any one or more feature.</p>
<p><img alt="" src="/home/isidzukuri/Desktop/ml_algorithms/images/Ridge-regression-cost-function-2.png" /></p>
<p><a href="https://www.ibm.com/topics/ridge-regression">https://www.ibm.com/topics/ridge-regression</a></p>
<p>Example 1:</p>
<p><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge_model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">ridge_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">ridge_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># the result</span>


<span class="c1"># computing of model performance metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span><span class="n">mean_squared_error</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span>
</pre></div>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a></p>
<p>Example 2 (model selects the best of proposed alpha coef):
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>

<span class="n">ridge_cv_model</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">),</span><span class="n">scoring</span><span class="o">=</span><span class="s">&#39;neg_mean_absolute_error&#39;</span><span class="p">)</span> <span class="c1"># it should pick the best alpha from given automaticaly</span>

<span class="n">ridge_cv_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">ridge_cv_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># the result</span>

<span class="c1"># computing of model performance metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span><span class="n">mean_squared_error</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span>
</pre></div></p>
<p><a href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity">https://scikit-learn.org/stable/modules/linear_model.html#ridge-complexity</a></p>
<h4 id="lasso">Lasso<a class="headerlink" href="#lasso" title="Permanent link"></a></h4>
<p>[regression]</p>
<p>is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason, Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients.</p>
<p>Lasso regression is ideal for predictive problems; its ability to perform automatic variable selection can simplify models and enhance prediction accuracy. That said, ridge regression may outperform lasso regression due to the amount of bias that lasso regression introduces by reducing coefficients towards zero. It also has its limitations with correlated features in the data as it arbitrarily chooses a feature to include in the model.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>

<span class="n">lasso_cv_model</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">lasso_cv_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">lasso_cv_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="c1"># the result</span>

<span class="c1"># computing of model performance metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span><span class="n">mean_squared_error</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/linear_model.html#lasso">https://scikit-learn.org/stable/modules/linear_model.html#lasso</a></p>
<p><a href="https://www.ibm.com/topics/lasso-regression">https://www.ibm.com/topics/lasso-regression</a></p>
<h4 id="elastic-net">Elastic Net<a class="headerlink" href="#elastic-net" title="Permanent link"></a></h4>
<p>[regression]</p>
<p>combines the penalties of ridge regression and lasso in an attempt to get the best of both.</p>
<p>ElasticNet is a linear regression model trained with both l1 and l2-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of l1 and l2 using the l1_ratio parameter.</p>
<p>Elastic-net is useful when there are multiple features that are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.</p>
<p>A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge’s stability under rotation.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>

<span class="n">elastic_model</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">l1_ratio</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span><span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">tol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> 
<span class="c1"># l1_ratio: how much it is lasso?  0 = 100% ridge, 1 = 100% lasso</span>

<span class="n">elastic_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">elastic_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="c1"># the result</span>

<span class="c1"># computing of model performance metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span><span class="n">mean_squared_error</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">test_predictions</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/linear_model.html#elastic-net">https://scikit-learn.org/stable/modules/linear_model.html#elastic-net</a></p>
<h4 id="logistic-regression">Logistic regression<a class="headerlink" href="#logistic-regression" title="Permanent link"></a></h4>
<p>[classification]</p>
<p>widely used for binary classification tasks, such as identifying whether an email is spam or not and diagnosing diseases by assessing the presence or absence of specific conditions based on patient test results. This approach utilizes the logistic (or sigmoid) function to transform a linear combination of input features into a probability value ranging between 0 and 1. This probability indicates the likelihood that a given input corresponds to one of two predefined categories. The essential mechanism of logistic regression is grounded in the logistic function&rsquo;s ability to model the probability of binary outcomes accurately. With its distinctive S-shaped curve, the logistic function effectively maps any real-valued number to a value within the 0 to 1 interval. This feature renders it particularly suitable for binary classification tasks, such as sorting emails into &ldquo;spam&rdquo; or &ldquo;not spam&rdquo;. By calculating the probability that the dependent variable will be categorized into a specific group, logistic regression provides a probabilistic framework that supports informed decision-making.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/Exam_pass_logistic_curve.svg" height="370"></p>
<p>Binary problem example:</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scaled_X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">log_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">log_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">log_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaled_X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p>Multiclass problem example:</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">scaled_X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Depending on warnings you may need to adjust max iterations allowed </span>
<span class="c1"># Or experiment with different solvers</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s">&#39;saga&#39;</span><span class="p">,</span><span class="n">multi_class</span><span class="o">=</span><span class="s">&quot;ovr&quot;</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="c1"># Penalty Type</span>
<span class="n">penalty</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;l1&#39;</span><span class="p">,</span> <span class="s">&#39;l2&#39;</span><span class="p">]</span>

<span class="c1"># Use logarithmically spaced C values (recommended in official docs)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">grid_model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">log_model</span><span class="p">,</span><span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;C&#39;</span><span class="p">:</span><span class="n">C</span><span class="p">,</span><span class="s">&#39;penalty&#39;</span><span class="p">:</span><span class="n">penalty</span><span class="p">})</span>
<span class="n">grid_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span><span class="p">,</span><span class="n">plot_confusion_matrix</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">grid_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaled_X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span><span class="p">,</span><span class="n">plot_confusion_matrix</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p><a href="https://en.wikipedia.org/wiki/Logistic_regression">https://en.wikipedia.org/wiki/Logistic_regression</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a></p>
<p><a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a></p>
<h3 id="knn-algorithm">KNN algorithm<a class="headerlink" href="#knn-algorithm" title="Permanent link"></a></h3>
<p>[classification, regression]</p>
<p><img alt="" src="/home/isidzukuri/Desktop/ml_algorithms/images/knn.gif" /></p>
<p>The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. It is one of the popular and simplest classification and regression classifiers used in machine learning today.</p>
<p>While there are several distance measures that you can choose from, this article will only cover the following:</p>
<ul>
<li><strong>Euclidean distance</strong> (p=2): This is the most commonly used distance measure, and it is limited to real-valued vectors. Using the below formula, it measures a straight line between the query point and the other point being measured.</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/knn_d1.png" height="250"></p>
<ul>
<li><strong>Manhattan distance</strong> (p=1): This is also another popular distance metric, which measures the absolute value between two points. It is also referred to as taxicab distance or city block distance as it is commonly visualized with a grid, illustrating how one might navigate from one address to another via city streets.</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/knn_d2.png" height="250"></p>
<ul>
<li><strong>Minkowski distance</strong> : This distance measure is the generalized form of Euclidean and Manhattan distance metrics. The parameter, p, in the formula below, allows for the creation of other distance metrics. Euclidean distance is represented by this formula when p is equal to two, and Manhattan distance is denoted with p equal to one.</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/knn_d3.png" height="250"></p>
<ul>
<li><strong>Hamming distance</strong> : This technique is used typically used with Boolean or string vectors, identifying the points where the vectors do not match. As a result, it has also been referred to as the overlap metric. This can be represented with the following formula:</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/knn_d4.png" height="250"></p>
<div class="highlight"><pre><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]])</span> <span class="c1"># [0]</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]])</span> <span class="c1"># [[0.666... 0.333...]]</span>
</pre></div>

<p><a href="https://www.ibm.com/topics/knn">https://www.ibm.com/topics/knn</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a></p>
<h4 id="support-vector-machines">Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permanent link"></a></h4>
<p>[classification, regression, outliers detection]</p>
<p>are a set of supervised learning methods used for classification, regression and outliers detection.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/svm.png">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/svc2.png"></p>
<p>The advantages of support vector machines are:</p>
<ul>
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
<li>Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>
</ul>
<p>The disadvantages of support vector machines include:</p>
<ul>
<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
</ul>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;C&#39;</span><span class="p">:[</span><span class="mf">0.001</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="c1">#tune here. Generate with `np.arange(0.6, 0.8, 0.01)`</span>
              <span class="s">&#39;gamma&#39;</span><span class="p">:[</span><span class="s">&#39;scale&#39;</span><span class="p">,</span><span class="s">&#39;auto&#39;</span><span class="p">],</span> 
              <span class="s">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s">&#39;linear&#39;</span><span class="p">,</span> <span class="s">&#39;poly&#39;</span><span class="p">,</span> <span class="s">&#39;rbf&#39;</span><span class="p">,</span> <span class="s">&#39;sigmoid&#39;</span><span class="p">],</span>
              <span class="s">&#39;class_weight&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s">&#39;balanced&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">]}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;Best hyper params: {grid.best_params_}&quot;</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scaled_X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a></p>
<h4 id="decision-tree">Decision tree<a class="headerlink" href="#decision-tree" title="Permanent link"></a></h4>
<p>[classification, regression]</p>
<p><strong>Classification criteria</strong>:</p>
<ul>
<li>Gini</li>
<li>Log Loss or Entropy</li>
</ul>
<p><strong>Regression criteria</strong>:</p>
<ul>
<li>Mean Squared Error</li>
<li>Half Poisson deviance</li>
<li>Mean Absolute Error</li>
</ul>
<p>Tree algorithms: ID3, C4.5, C5.0 and CART </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/sphx_glr_plot_tree_regression_001.png"></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/tree_plot.jpg"></p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html</a></p>
<h4 id="random-forest">Random Forest<a class="headerlink" href="#random-forest" title="Permanent link"></a></h4>
<p>[classification, regression]</p>
<p>combines the output of multiple decision trees to reach a single result. It handles both classification and regression problems. </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/random_forest.png" height="410px"></p>
<p>Classification example:
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">n_estimators</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">200</span><span class="p">]</span>
<span class="n">max_features</span><span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">bootstrap</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span><span class="bp">False</span><span class="p">]</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;n_estimators&#39;</span><span class="p">:</span><span class="n">n_estimators</span><span class="p">,</span>
             <span class="s">&#39;max_features&#39;</span><span class="p">:</span><span class="n">max_features</span><span class="p">,</span>
             <span class="s">&#39;bootstrap&#39;</span><span class="p">:</span><span class="n">bootstrap</span><span class="p">}</span>

<span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">rfc</span><span class="p">,</span><span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span> <span class="c1">#{&#39;bootstrap&#39;: True, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 64}</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div></p>
<p><strong>How to pick right number of trees?</strong></p>
<p>Start with grid search and fine tune with plots. After certain quantity of trees in forest accuracy or amount of misclassifications(errors) will reach threshold,
there will be no sense to add more trees. It is visible on plots:</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">misclassifications</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rfc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">rfc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">err</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">n_missed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">!=</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># watch the video to understand this line!!</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">misclassifications</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_missed</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span><span class="n">errors</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span><span class="n">misclassifications</span><span class="p">)</span>
</pre></div>

<p>Regression example:
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">trees</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">trees</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span>
<span class="n">MAE</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">)</span>
<span class="n">MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">)</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="p">)</span> <span class="c1">#import numpy as np</span>
</pre></div></p>
<p><a href="https://www.ibm.com/topics/random-forest">https://www.ibm.com/topics/random-forest</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>
<h4 id="adaboost">AdaBoost<a class="headerlink" href="#adaboost" title="Permanent link"></a></h4>
<p>[classification, regression]</p>
<p>An AdaBoost <strong>classifier</strong> is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</p>
<p>An AdaBoost <strong>regressor</strong> is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/ada_boost_process.jpg"></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/ada_boost_process_2.jpg"></p>
<p>Classifier example:
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html</a></p>
<h4 id="gradient-boosting">Gradient boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link"></a></h4>
<p>[classification, regression]</p>
<p>is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/gradient_boosting.png" height="350px"></p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;n_estimators&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span><span class="s">&#39;max_depth&#39;</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">()</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">param_grid</span><span class="p">)</span>

<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_preds</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html</a></p>
<h4 id="text-classification">Text classification<a class="headerlink" href="#text-classification" title="Permanent link"></a></h4>
<div class="highlight"><pre><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;airline_sentiment&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;text&#39;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># vectorization</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s">&#39;english&#39;</span><span class="p">)</span>
<span class="n">tfidf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>


<span class="c1"># train model on vector. </span>
<span class="c1"># Its possible to use any classifier. For example: MultinomialNB, LogisticRegression, LinearSVC...</span>

<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="n">nb</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_tfidf</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_tfidf</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;accuracy_score: {accuracy_score(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">&quot;confusion_matrix: </span><span class="se">\r\n</span><span class="s">{confusion_matrix(y_test,y_pred)}&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html</a></p>
<h3 id="unsupervised-learning">Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Permanent link"></a></h3>
<p>in artificial intelligence is a type of machine learning that learns from data without human supervision.</p>
<h4 id="k-means-clustering">k-means clustering<a class="headerlink" href="#k-means-clustering" title="Permanent link"></a></h4>
<p>[clustering]</p>
<p>Distance based. K-means is an unsupervised learning method for clustering data points. The algorithm iteratively divides data points into K clusters by minimizing the variance in each cluster. <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualization</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/k_means.gif" height="300"></p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">scaled_X</span><span class="p">)</span>

<span class="n">X</span><span class="p">[</span><span class="s">&#39;Cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_labels</span>
</pre></div>

<p><strong>Choosing K Value</strong></p>
<p><div class="highlight"><pre><span class="n">ssd</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_X</span><span class="p">)</span>
    <span class="c1">#Sum of squared distances of samples to their closest cluster center.</span>
    <span class="n">ssd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="n">ssd</span><span class="p">,</span><span class="s">&#39;o--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;K Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot; Sum of Squared Distances&quot;</span><span class="p">)</span>
</pre></div>
When diff is smallest, <code>6</code> looks good here:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/k_means_diff.jpg" height="300"></p>
<div class="highlight"><pre><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ssd</span><span class="p">)</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="c1"># 0             NaN</span>
<span class="c1"># 1   -99004.700248</span>
<span class="c1"># 2   -99285.008576</span>
<span class="c1"># 3   -50373.800535</span>
<span class="c1"># 4   -76061.758683</span>
<span class="c1"># 5   -12598.390287</span>
<span class="c1"># 6   -93007.868047</span>
<span class="c1"># 7   -33768.137795</span>
</pre></div>

<p><a href="https://www.w3schools.com/python/python_ml_k-means.asp">https://www.w3schools.com/python/python_ml_k-means.asp</a></p>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering">https://en.wikipedia.org/wiki/K-means_clustering</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a></p>
<h4 id="dbscan">DBSCAN<a class="headerlink" href="#dbscan" title="Permanent link"></a></h4>
<p>(Density-Based Spatial Clustering of Applications with Noise), captures the insight that clusters are dense groups of points. The idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.</p>
<p>It works like this: First we choose two parameters, a positive number epsilon and a natural number minPoints. We then begin by picking an arbitrary point in our dataset. If there are more than minPoints points within a distance of epsilon from that point, (including the original point itself), we consider all of them to be part of a &ldquo;cluster&rdquo;. We then expand that cluster by checking all of the new points and seeing if they too have more than minPoints points within a distance of epsilon, growing the cluster recursively if so.</p>
<p>Eventually, we run out of points to add to the cluster. We then pick a new arbitrary point and repeat the process. Now, it&rsquo;s entirely possible that a point we pick has fewer than minPoints points in its epsilon ball, and is also not a part of any other cluster. If that is the case, it&rsquo;s considered a &ldquo;noise point&rdquo; not belonging to any cluster. 
<a href="https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/">Visualization</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/dbscan.jpg"></p>
<p>0 = first categiry, 1 = second category, -1 = ouyliers</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>

<p><code>eps</code> - the maximum distance between two samples for one to be considered as in the neighborhood of the other. The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</p>
<p><code>min_samples</code> - the number of samples (or total weight) in a neighborhood for a point to be considered as a core point.
<strong>How to pick <code>min_samples</code></strong>:
<code>min_samples</code> starting point may be two times number of dimensions of data.</p>
<p><code>metric</code> - the metric to use when calculating distance between instances in a feature array.</p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html</a></p>
<h4 id="hotspots">Hotspots<a class="headerlink" href="#hotspots" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/hotspots.jpg" height="420"></p>
<h4 id="random-cut-forest">Random Cut Forest<a class="headerlink" href="#random-cut-forest" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/random_cut_forest.jpg" height="420"></p>
<h3 id="reinforcement-learning">Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link"></a></h3>
<p>Range of techniques based on expierience. In its most general form a reinforcement learning algorithm has three components:</p>
<ul>
<li>an exploration strategy for trying different actions</li>
<li>reinforcement function that gives feedback on how good each action is</li>
<li>learning rule that links two above together</li>
</ul>
<p>Reinforcement learning deals with a unique problem setup where an arbitrary agent is trying to learn the optimal way of interacting with an environment. In return for its actions, it receives rewards; the agent’s goal is to find an optimal policy that maximizes cumulative numerical return.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/rl_cycle.png" height="420"></p>
<p><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">https://en.wikipedia.org/wiki/Reinforcement_learning</a></p>
<h4 id="a-taxonomy-of-rl-algorithms">A Taxonomy of RL Algorithms<a class="headerlink" href="#a-taxonomy-of-rl-algorithms" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/rl_algorithms_9_15.svg" height="420"></p>
<p><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms</a></p>
<h4 id="q-learning">Q learning<a class="headerlink" href="#q-learning" title="Permanent link"></a></h4>
<p>Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence &ldquo;model-free&rdquo;), and it can handle problems with stochastic transitions and rewards without requiring adaptations.</p>
<p>For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. &ldquo;Q&rdquo; refers to the function that the algorithm computes – the expected rewards for an action taken in a given state.</p>
<p>Fits for state machines, decision trees.</p>
<h3 id="neural-networks">Neural networks<a class="headerlink" href="#neural-networks" title="Permanent link"></a></h3>
<p>is a model inspired by the structure and function of biological neural networks in animal brains.</p>
<p>An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The &ldquo;signal&rdquo; is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. </p>
<p>Exemples of NN topography</p>
<p><img alt="example of NN" src="/home/isidzukuri/Desktop/ml_algorithms/images/nn.png" />
<img alt="example of NN" src="/home/isidzukuri/Desktop/ml_algorithms/images/nn.gif" /></p>
<p>An <strong>activation function</strong> in the context of neural networks is a mathematical function applied to the output of a neuron. The purpose of an activation function is to introduce non-linearity into the model, allowing the network to learn and represent complex patterns in the data. Without non-linearity, a neural network would essentially behave like a linear regression model, regardless of the number of layers it has.</p>
<p>The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. </p>
<ul>
<li><a href="https://www.geeksforgeeks.org/activation-functions-neural-networks/">https://www.geeksforgeeks.org/activation-functions-neural-networks/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Activation_function">https://en.wikipedia.org/wiki/Activation_function</a></li>
<li><a href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464">https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464</a></li>
<li><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/isidzukuri/nn_discovery" title="GitHub Repository: isidzukuri/nn_discovery">isidzukuri/nn_discovery</a></li>
</ul>
<h4 id="optimizer">Optimizer<a class="headerlink" href="#optimizer" title="Permanent link"></a></h4>
<p>Tells your model how to update its internal parameters to best lower the loss.</p>
<p><a href="https://www.learnpytorch.io/01_pytorch_workflow/">https://www.learnpytorch.io/01_pytorch_workflow/</a></p>
<h4 id="how-to-pick-lossoptimizer-function">How to pick loss/optimizer function?<a class="headerlink" href="#how-to-pick-lossoptimizer-function" title="Permanent link"></a></h4>
<p>Table of various loss functions and optimizers, there are more but these some common ones:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/loss_optimizer_table.jpg"></p>
<p><a href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a></p>
<p><a href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a></p>
<h4 id="learning-rate">learning rate<a class="headerlink" href="#learning-rate" title="Permanent link"></a></h4>
<p>lr is the learning rate you&rsquo;d like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it&rsquo;s set by a machine learning engineer). Common starting values for the learning rate are 0.01, 0.001, 0.0001, however, these can also be adjusted over time</p>
<h4 id="raw-logits-predictions-probabilities-predictions-labels">Raw logits -&gt; Predictions probabilities -&gt; Predictions labels<a class="headerlink" href="#raw-logits-predictions-probabilities-predictions-labels" title="Permanent link"></a></h4>
<p>model outputs are going to be <strong>logits</strong></p>
<p><strong>logits</strong> can be converted into <strong>prediction_probabilities</strong> by passing them to activation function</p>
<p>Then <strong>prediction_probabilities</strong> can be converted to <strong>prediction_labels</strong></p>
<h4 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Permanent link"></a></h4>
<p>Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.</p>
<p>Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.</p>
<p><img alt="Softmax example" src="/home/isidzukuri/Desktop/ml_algorithms/images/softmax-example_orig.png" /></p>
<p><a href="https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax">https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax</a></p>
<h4 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link"></a></h4>
<p>Gradient Descent stands as a cornerstone orchestrating the intricate dance of model optimization. At its core, it is a numerical optimization algorithm that aims to find the optimal parameters—weights and biases—of a neural network by minimizing a defined cost function.</p>
<p>Gradient Descent (GD) is a widely used optimization algorithm in machine learning and deep learning that minimises the cost function of a neural network model during training. It works by iteratively adjusting the weights or parameters of the model in the direction of the negative gradient of the cost function until the minimum of the cost function is reached.</p>
<p>The learning happens during the backpropagation while training the neural network-based model. There is a term known as Gradient Descent, which is used to optimize the weight and biases based on the cost function. The cost function evaluates the difference between the actual and predicted outputs.</p>
<p><a href="https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/">https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/</a></p>
<h4 id="backpropagation">Backpropagation<a class="headerlink" href="#backpropagation" title="Permanent link"></a></h4>
<p>The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model’s parameters (weights and biases).</p>
<p>Backpropagation is a gradient estimation method used to train neural network models. The gradient estimate is used by the optimization algorithm to compute the network parameter updates. </p>
<p><a href="https://en.wikipedia.org/wiki/Backpropagation">https://en.wikipedia.org/wiki/Backpropagation</a></p>
<h4 id="regularization-in-neural-networks">Regularization in Neural Networks<a class="headerlink" href="#regularization-in-neural-networks" title="Permanent link"></a></h4>
<ul>
<li><strong>Dropout</strong> regularizes neural networks by randomly dropping out nodes, along with their input and output connections, from the network during training. Dropout trains several variations of a fixed-sized architecture, with each variation having different randomized nodes left out of the architecture.</li>
<li><strong>Weight decay</strong></li>
<li><strong>Learning rate warmup</strong></li>
<li><strong>Learning rate decay</strong></li>
<li><strong>Gradient clipping</strong></li>
<li><strong>Label smoothing</strong></li>
</ul>
<h4 id="convolutional-neural-networks-cnn">Convolutional neural networks (CNN)<a class="headerlink" href="#convolutional-neural-networks-cnn" title="Permanent link"></a></h4>
<p>is the extended version of artificial neural networks (ANN) which is predominantly used to extract the feature from the grid-like matrix dataset. For example visual datasets like images or videos where data patterns play an extensive role.</p>
<p><img alt="CNN" src="/home/isidzukuri/Desktop/ml_algorithms/images/cnn.png" /></p>
<p>Practical Applications of CNNs:
- Image classification
- Object detection
- Facial recognition
- Optical flow (task of predicting movement between two images)</p>
<p><strong>CNN explainer</strong> <a href="https://poloclub.github.io/cnn-explainer/">https://poloclub.github.io/cnn-explainer/</a></p>
<h4 id="other-nn-architectures">Other NN architectures<a class="headerlink" href="#other-nn-architectures" title="Permanent link"></a></h4>
<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>
<h4 id="optical-flow">Optical Flow<a class="headerlink" href="#optical-flow" title="Permanent link"></a></h4>
<p>Optical flow is the task of predicting movement between two images, usually two consecutive frames of a video. Optical flow models take two images as input, and predict a flow: the flow indicates the displacement of every single pixel in the first image, and maps it to its corresponding pixel in the second image. </p>
<p><a href="https://www.youtube.com/watch?v=YRhxdVk_sIs">https://www.youtube.com/watch?v=YRhxdVk_sIs</a></p>
<p><a href="https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns">https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns</a></p>
<p><a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/">https://www.geeksforgeeks.org/introduction-convolution-neural-network/</a></p>
<h4 id="transfer-learning">Transfer learning<a class="headerlink" href="#transfer-learning" title="Permanent link"></a></h4>
<p>Transferring information from one machine learning task to another. </p>
<p>Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.</p>
<p>Options:</p>
<ul>
<li>
<p>use pretrained model as it is</p>
</li>
<li>
<p>use pretrained model and weights on different training dataset + adding/editing some layers</p>
</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/06-effnet-b0-feature-extractor.png"></p>
<p>Input data should go thru the same transformations as for original model.</p>
<p>The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the <code>features</code> section) and then adjust the output layers (also called <code>head/classifier</code> layers) to suit your needs.</p>
<p><a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">https://www.learnpytorch.io/06_pytorch_transfer_learning/</a></p>
<h4 id="which-pretrained-model-should-be-chosen">Which pretrained model should be chosen<a class="headerlink" href="#which-pretrained-model-should-be-chosen" title="Permanent link"></a></h4>
<p>It depends on your problem/the device you&rsquo;re working with.</p>
<p>Generally, the higher number in the model name (e.g. efficientnet_b0() -&gt; efficientnet_b1() -&gt; efficientnet_b7()) means better performance but a larger model.</p>
<p>You might think better performance is always better, right?</p>
<p>That&rsquo;s true but some better performing models are too big for some devices.</p>
<p>For example, say you&rsquo;d like to run your model on a mobile-device, you&rsquo;ll have to take into account the limited compute resources on the device, thus you&rsquo;d be looking for a smaller model.</p>
<p>But if you&rsquo;ve got unlimited compute power, as The Bitter Lesson states, you&rsquo;d likely take the biggest, most compute hungry model you can.</p>
<p>Understanding this performance vs. speed vs. size tradeoff will come with time and practice.</p>
<h4 id="recipe-for-training-nn">Recipe for training NN<a class="headerlink" href="#recipe-for-training-nn" title="Permanent link"></a></h4>
<p><a href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a></p>
<h3 id="more-algorithms">More algorithms<a class="headerlink" href="#more-algorithms" title="Permanent link"></a></h3>
<ul>
<li>
<p>AWS built-in algorithms or pretrained models: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>
</li>
<li>
<p><a href="https://scikit-learn.org/stable/user_guide">https://scikit-learn.org/stable/user_guide</a>.</p>
</li>
</ul>
<h3 id="machine-learning-components">machine learning components<a class="headerlink" href="#machine-learning-components" title="Permanent link"></a></h3>
<p><a href="https://paperswithcode.com/methods">https://paperswithcode.com/methods</a></p>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link"></a></h2>
<p>is a set of methods for reducing overfitting in machine learning models. Typically, regularization trades a marginal decrease in training accuracy for an increase in generalizability. Prevents overfitting.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/under_over_fit.png"></p>
<p><a href="https://www.ibm.com/topics/regularization">https://www.ibm.com/topics/regularization</a></p>
<p><a href="https://www.geeksforgeeks.org/regularization-in-machine-learning/">https://www.geeksforgeeks.org/regularization-in-machine-learning/</a></p>
<p>The commonly used regularization with linear models techniques are: </p>
<ul>
<li>Lasso Regularization – L1 Regularization</li>
<li>Ridge Regularization – L2 Regularization</li>
<li>Elastic Net Regularization – L1 and L2 Regularization</li>
</ul>
<h3 id="types-of-regularization-in-machine-learning">Types of regularization in machine learning<a class="headerlink" href="#types-of-regularization-in-machine-learning" title="Permanent link"></a></h3>
<h4 id="dataset">Dataset<a class="headerlink" href="#dataset" title="Permanent link"></a></h4>
<ul>
<li><strong>Data augmentation</strong> is a regularization technique that modifies model training data. It expands the size of the training set by creating artificial data samples derived from pre-existing training data. Adding more samples to the training set, particularly of instances rare in real world data, exposes a model to a greater quantity and diversity of data from which it learns. </li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/augmentation.png"></p>
<p><a href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/">https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/</a></p>
<h4 id="model-training">Model training<a class="headerlink" href="#model-training" title="Permanent link"></a></h4>
<ul>
<li><strong>Early stopping</strong> is perhaps the most readily implemented regularization technique. In short, it limits the number of iterations during model training. Here, a model continuously passes through the training data, stopping once there is no improvement (and perhaps even deterioration) in training and validation accuracy. The goal is to train a model until it has reached the lowest possible training error preceding a plateau or increase in validation error.</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/early_stop.png"></p>
<h2 id="confusion-matrix">Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Permanent link"></a></h2>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/confusion_matrix.jpg" height="300"></p>
<h2 id="performance-metrics-in-machine-learning">Performance Metrics in Machine Learning<a class="headerlink" href="#performance-metrics-in-machine-learning" title="Permanent link"></a></h2>
<p>There are at least  7 different categories and types of metrics used to measure machine learning performance, including: </p>
<ol>
<li>Classification Metrics</li>
</ol>
<ul>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
<li>Logarithmic Loss</li>
<li>F1-Score</li>
<li>Receiver Operating Characteristic  </li>
<li>Area Under Curve</li>
</ul>
<ol start="2">
<li>Regression Metrics</li>
</ol>
<ul>
<li>Mean Squared Error</li>
<li>Mean Absolute Error</li>
</ul>
<ol start="3">
<li>Ranking Metrics</li>
</ol>
<ul>
<li>Mean Reciprocal Rate</li>
<li>Discounted Cumulative Gain</li>
<li>Non-Discounted Cumulative Gain</li>
</ul>
<ol start="4">
<li>Statistical Metrics</li>
</ol>
<ul>
<li>Correlation</li>
</ul>
<ol start="5">
<li>Computer Vision Metrics</li>
</ol>
<ul>
<li>Peak Signal-to-Noise</li>
<li>Structural Similarity Index</li>
<li>Intersection over Union</li>
</ul>
<ol start="6">
<li>Natural Language Processing Metrics</li>
</ol>
<ul>
<li>Perplexity</li>
<li>Bilingual Evaluation Understudy Score</li>
</ul>
<ol start="7">
<li>Deep Learning Related Metrics</li>
</ol>
<ul>
<li>Inception Score</li>
<li>Frechet Inception Distance</li>
</ul>
<h4 id="accuracy">Accuracy<a class="headerlink" href="#accuracy" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/accuracy.jpg"></p>
<h4 id="precision">Precision<a class="headerlink" href="#precision" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/precision.jpg"></p>
<p>What proportion of positive identifications was actually correct</p>
<h4 id="recall">Recall<a class="headerlink" href="#recall" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/recall.jpg"></p>
<p>What proportion of actual positives was identified correctly</p>
<p>* The <strong>tradeoff between precision and recall</strong> occurs because increasing the threshold for classification will result in fewer false positives (increasing precision) but also more false negatives (decreasing recall), and vice versa.</p>
<h4 id="f1-score">F1 score<a class="headerlink" href="#f1-score" title="Permanent link"></a></h4>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/f1.jpg"></p>
<p>The range for F1 scores is between 0 and 1, with one being the absolute best score possible. Naturally then, the higher the F1 score, the better, with a poor score denoting both low precision and low recall. As your recall and precision scores increase, your F1 score will also increase. That means if you find  your F1 score is low, a good place to start looking for solutions is within your precision or recall metrics.  </p>
<p><a href="https://www.v7labs.com/blog/performance-metrics-in-machine-learning">https://www.v7labs.com/blog/performance-metrics-in-machine-learning</a></p>
<p><a href="https://www.aiacceleratorinstitute.com/evaluating-machine-learning-models-metrics-and-techniques/">https://www.aiacceleratorinstitute.com/evaluating-machine-learning-models-metrics-and-techniques/</a></p>
<p><a href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall">https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall</a></p>
<p><a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a></p>
<h2 id="ensemble-methods">Ensemble methods<a class="headerlink" href="#ensemble-methods" title="Permanent link"></a></h2>
<p>In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</p>
<p>Ensemble means ‘a collection of things’ and in Machine Learning terminology, Ensemble learning refers to the approach of combining multiple ML models to produce a more accurate and robust prediction compared to any individual model. It implements an ensemble of fast algorithms (classifiers) such as decision trees for learning and allows them to vote.</p>
<p>There are many types of ensembles. For example:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/bagging_boosting.jpg" height="300"></p>
<p><a href="https://en.wikipedia.org/wiki/Ensemble_learning">https://en.wikipedia.org/wiki/Ensemble_learning</a></p>
<p><a href="https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/">https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/</a></p>
<h2 id="training-data">Training data<a class="headerlink" href="#training-data" title="Permanent link"></a></h2>
<h3 id="sampling">Sampling<a class="headerlink" href="#sampling" title="Permanent link"></a></h3>
<p>in ML involves selecting a subset of data from a larger dataset, ensuring that the smaller dataset accurately represents the larger one.</p>
<p>Types of sampling:</p>
<ul>
<li>
<p><strong>Random Sampling</strong>: Random sampling is the simplest form of sampling in machine learning. It involves selecting data points randomly from a dataset with no specific pattern. This method assumes that every data point in the dataset has an equal chance of being selected. </p>
</li>
<li>
<p><strong>Non probability sampling</strong>(convinience, snowball, judgment, quota) is when the selection of data is not basd on ny probability criteria.</p>
</li>
<li>
<p><strong>Simple random sampling</strong>: you give all samples in population equal probabilities of being selected. For example, you randomly select 10% of the population, giving all members of this population an equal chance 10% of being selected.</p>
</li>
<li>
<p><strong>Stratified sampling</strong>: you can first divide popultion into th groups and sample from each group separetly.</p>
</li>
<li>
<p><strong>Weighed sampling</strong>: each sample is given a weight, which determines the probabiliti of it beign selected. For example you have three samples A, B, C, and want them to be selected with probability 50%, 30%, 20%, you can give them weights 0.5, 0.3, 0.2.</p>
</li>
<li>
<p><strong>Reservoir sampling</strong>: is used for randomly sampling k items from a stream of data of unknown size, in a single pass.</p>
</li>
<li>
<p><strong>Importance sampling</strong></p>
</li>
</ul>
<h3 id="resampling">Resampling<a class="headerlink" href="#resampling" title="Permanent link"></a></h3>
<p>Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/resampling.jpg"></p>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
<h3 id="cost-sensitive-learning">Cost-sensitive learning<a class="headerlink" href="#cost-sensitive-learning" title="Permanent link"></a></h3>
<p>is a subfield of machine learning that takes the costs of prediction errors (and potentially other costs) into account when training a machine learning model. It is a field of study that is closely related to the field of imbalanced learning that is concerned with classification on datasets with a skewed class distribution.</p>
<p>This method diverges from traditional approaches by introducing a cost matrix, explicitly specifying the penalties or benefits for each type of prediction error. The inherent difficulty which cost-sensitive machine learning tackles is that minimizing different kinds of classification errors is a multi-objective optimization problem. </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/cost_sensitive_learning.jpg"></p>
<h3 id="focal-loss">Focal Loss<a class="headerlink" href="#focal-loss" title="Permanent link"></a></h3>
<p>function addresses class imbalance during training in tasks like object detection. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. </p>
<p><a href="https://paperswithcode.com/method/focal-loss">https://paperswithcode.com/method/focal-loss</a></p>
<h3 id="data-lineage">Data lineage<a class="headerlink" href="#data-lineage" title="Permanent link"></a></h3>
<p>is the story behind the data. It tracks the data from its creation point to the points of consumption. This pipeline of dataflow involves input and output points, transformation and modeling processes the data has undergone, record analysis, visualizations, and several other processes which are constantly tracked and updated.</p>
<p>The objective of data lineage is to observe the entire lifecycle of data such that the pipeline can be upgraded and leveraged for optimal performance.</p>
<p>A visual representation can provide transparency to the flow of data from its source systems through transformation, processing, and aggregation steps and into analysis, allowing data engineers to drill down on specific details or check versions and changes over time.</p>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
<p><a href="https://www.linkedin.com/pulse/types-sampling-machine-learning-chirag-subramanian-hnsoc/">https://www.linkedin.com/pulse/types-sampling-machine-learning-chirag-subramanian-hnsoc/</a></p>
<h2 id="feature-engeneering">Feature engeneering<a class="headerlink" href="#feature-engeneering" title="Permanent link"></a></h2>
<h3 id="dealing-with-outliers">Dealing with Outliers<a class="headerlink" href="#dealing-with-outliers" title="Permanent link"></a></h3>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/outlier.gif" height='420'></p>
<p>In statistics, an outlier is a data point that differs significantly from other observations.An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.</p>
<p>Remember that even if a data point is an outlier, its still a data point! Carefully consider your data, its sources, and your goals whenver deciding to remove an outlier. Each case is different!</p>
<p>How to detect:</p>
<ul>
<li>
<p>visualization tools(Box plot, Scatter plot&hellip;)</p>
</li>
<li>
<p>mathematical functions(Z-Score, IQR score&hellip;)</p>
</li>
</ul>
<p>How to solve:</p>
<ul>
<li>
<p>correct</p>
</li>
<li>
<p>remove</p>
</li>
</ul>
<p><a href="https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/">scikit-learn course, Section 11: Feature Engineering and Data Preparation</a></p>
<p><a href="https://en.wikipedia.org/wiki/Outlier">https://en.wikipedia.org/wiki/Outlier</a></p>
<p><a href="https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm">https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm</a></p>
<p><a href="https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba">https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba</a></p>
<h3 id="dealing-with-missing-data">Dealing with Missing Data<a class="headerlink" href="#dealing-with-missing-data" title="Permanent link"></a></h3>
<h4 id="removing-features-or-removing-rows">Removing Features or Removing Rows<a class="headerlink" href="#removing-features-or-removing-rows" title="Permanent link"></a></h4>
<p>If only a few rows relative to the size of your dataset are missing some values, then it might just be a good idea to drop those rows. What does this cost you in terms of performace? It essentialy removes potential training/testing data, but if its only a few rows, its unlikely to change performance.</p>
<p>Sometimes it is a good idea to remove a feature entirely if it has too many null values. However, you should carefully consider why it has so many null values, in certain situations null could just be used as a separate category.</p>
<p>Take for example a feature column for the number of cars that can fit into a garage. Perhaps if there is no garage then there is a null value, instead of a zero. It probably makes more sense to quickly fill the null values in this case with a zero instead of a null. Only you can decide based off your domain expertise and knowledge of the data set!</p>
<h4 id="filling-in-data-or-dropping-data">Filling in Data or Dropping Data?<a class="headerlink" href="#filling-in-data-or-dropping-data" title="Permanent link"></a></h4>
<p>Choose some threshold where we decide it is ok to drop a row if its missing some data (instead of attempting to fill in that missing data point). We will choose 1% as our threshold. This means if less than 1% of the rows are missing this feature, we will consider just dropping that row, instead of dealing with the feature itself. There is no right answer here, just use common sense and your domain knowledge of the dataset, obviously you don&rsquo;t want to drop a very high threshold like 50% , you should also explore correlation to the dataset, maybe it makes sense to drop the feature instead.</p>
<p>Sometimes you may want to take the approach that above a certain missing percentage threshold, you will simply remove the feature from all the data. For example if 99% of rows are missing a feature, it will not be predictive, since almost all the data does not have any value for it.</p>
<h4 id="imputation-of-missing-data">Imputation of Missing Data<a class="headerlink" href="#imputation-of-missing-data" title="Permanent link"></a></h4>
<p>To impute missing data, we need to decide what other filled in (no NaN values) feature most probably relates and is correlated with the missing feature data. Example:</p>
<div class="highlight"><pre><span class="c1"># Neighborhood: Physical locations within Ames city limits</span>
<span class="c1"># LotFrontage: Linear feet of street connected to property</span>
<span class="c1"># We will operate under the assumption that the Lot Frontage is related to what neighborhood a house is in.</span>

<span class="n">df</span><span class="p">[</span><span class="s">&#39;Lot Frontage&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#39;Neighborhood&#39;</span><span class="p">)[</span><span class="s">&#39;Lot Frontage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">val</span><span class="p">:</span> <span class="n">val</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/modules/impute.html">https://scikit-learn.org/stable/modules/impute.html</a></p>
<h3 id="dealing-with-categorical-data">Dealing with Categorical Data<a class="headerlink" href="#dealing-with-categorical-data" title="Permanent link"></a></h3>
<p>Many machine learning models can not deal with categorical data set as strings. For example linear regression can not apply a a Beta Coefficent to colors like &ldquo;red&rdquo; or &ldquo;blue&rdquo;. Instead we need to convert these categories into &ldquo;dummy&rdquo; variables, otherwise known as &ldquo;one-hot&rdquo; encoding.</p>
<p>how to solve:</p>
<ul>
<li>
<p>One Hot Encoding</p>
</li>
<li>
<p>Ordinal Encoding</p>
</li>
<li>
<p>Frequency Encoding</p>
</li>
<li>
<p>Target Encoding</p>
</li>
<li>
<p>Probability Ratio Encoding</p>
</li>
<li>
<p>Weight of Evidence Encoder</p>
</li>
<li>
<p>Binning</p>
</li>
</ul>
<p><a href="https://medium.com/geekculture/feature-engineering-for-categorical-data-a77a04b3308">https://medium.com/geekculture/feature-engineering-for-categorical-data-a77a04b3308</a></p>
<p><a href="https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1">https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1</a></p>
<p><a href="https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/">scikit-learn course, Section 11: Feature Engineering and Data Preparation</a></p>
<h3 id="scaling">Scaling<a class="headerlink" href="#scaling" title="Permanent link"></a></h3>
<p>is a technique to standardize the independent features present in the data in a fixed range.</p>
<p>It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.</p>
<p>Any mathematical transform or technique that shifts the range of a label and/or feature value.</p>
<p>Common forms of scaling useful in Machine Learning include:</p>
<ul>
<li>
<p>linear scaling, which typically uses a combination of subtraction and division to replace the original value with a number between -1 and +1 or between 0 and 1.</p>
</li>
<li>
<p>logarithmic scaling, which replaces the original value with its logarithm.</p>
</li>
<li>
<p>Z-score normalization, which replaces the original value with a floating-point value representing the number of standard deviations from that feature&rsquo;s mean.</p>
</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/normalizations-at-a-glance-v2.svg"></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/norm-log-scaling-movie-ratings.svg"></p>
<p>scikit-learn code:
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div></p>
<p><a href="https://www.geeksforgeeks.org/ml-feature-scaling-part-2/">https://www.geeksforgeeks.org/ml-feature-scaling-part-2/</a>
<a href="https://developers.google.com/machine-learning/glossary#scaling">https://developers.google.com/machine-learning/glossary#scaling</a></p>
<h3 id="feature-crossing">Feature crossing<a class="headerlink" href="#feature-crossing" title="Permanent link"></a></h3>
<p>is a technique to combine two or more features to generate new features.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/feature_crossing.jpg"></p>
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permanent link"></a></h3>
<p>is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">principal_components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaled_X</span><span class="p">)</span>
</pre></div>

<p><a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html">https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a></p>
<p><a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis">https://builtin.com/data-science/step-step-explanation-principal-component-analysis</a></p>
<h3 id="data-leakage">Data Leakage<a class="headerlink" href="#data-leakage" title="Permanent link"></a></h3>
<p>occurs when information from outside the training dataset is unintentionally utilized during the model creation process. This leakage can have detrimental effects on the model&rsquo;s predictions and its ability to generalize unseen data, resulting in unreliable and inaccurate predictions.</p>
<p>Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.</p>
<p>In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.</p>
<p>There are two main types of leakage: target leakage and train-test contamination.</p>
<p><a href="https://www.kaggle.com/code/alexisbcook/data-leakage">https://www.kaggle.com/code/alexisbcook/data-leakage</a></p>
<h3 id="feature-importance">Feature importance<a class="headerlink" href="#feature-importance" title="Permanent link"></a></h3>
<p>is a step in building a machine learning model that involves calculating the score for all input features in a model to establish the importance of each feature in the decision-making process. The higher the score for a feature, the larger effect it has on the model to predict a certain variable.</p>
<p>There are many method to measure feature importance. For example you can use SHAP.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/shap_1.jpg">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/shap_2.jpg">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/shap_header.svg"></p>
<p><a href="https://www.kaggle.com/code/wrosinski/shap-feature-importance-with-feature-engineering">https://www.kaggle.com/code/wrosinski/shap-feature-importance-with-feature-engineering</a></p>
<p><a href="https://shap.readthedocs.io/en/latest/">https://shap.readthedocs.io/en/latest/</a></p>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
<h2 id="performance">Performance<a class="headerlink" href="#performance" title="Permanent link"></a></h2>
<h3 id="batching">Batching<a class="headerlink" href="#batching" title="Permanent link"></a></h3>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/batch_queries.jpg"></p>
<h3 id="quantization">Quantization<a class="headerlink" href="#quantization" title="Permanent link"></a></h3>
<p>Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).</p>
<p>Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.</p>
<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization">https://huggingface.co/docs/optimum/concept_guides/quantization</a></p>
<h3 id="knowledge-distillation">Knowledge Distillation<a class="headerlink" href="#knowledge-distillation" title="Permanent link"></a></h3>
<p>Knowledge distillation is a technique that enables knowledge transfer from large, computationally expensive models to smaller ones without losing validity. This allows for deployment on less powerful hardware, making evaluation faster and more efficient.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/distillation_output_loss.png"></p>
<p><a href="https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html">https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html</a></p>
<h3 id="pruning">Pruning<a class="headerlink" href="#pruning" title="Permanent link"></a></h3>
<p>remove weights (unstructured) or entire channels (structured) to reduce the size of the network. The objective is to preserve the model’s performance while increasing its sparsity.</p>
<p><a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</a></p>
<h3 id="using-ml-to-optimize-ml-models">Using ML to optimize ML models<a class="headerlink" href="#using-ml-to-optimize-ml-models" title="Permanent link"></a></h3>
<p>Apache TVM is a compiler stack for deep learning systems. It is designed to close the gap between the productivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends. TVM works with deep learning frameworks to provide end to end compilation to different backends.</p>
<p>AutoTVM offers a way to tune models and operators by providing a template schedule, and searcing the parameter space defined by the template. These how-tos demonstrate how to write template schedules and optimize them for a variety of different hardware platforms. <a href="https://daobook.github.io/tvm/docs/how_to/tune_with_autotvm/index.html">Read more</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/ml_tune_ml.jpg"></p>
<p><a href="https://daobook.github.io/tvm/docs/how_to/tune_with_autotvm/index.html">https://daobook.github.io/tvm/docs/how_to/tune_with_autotvm/index.html</a></p>
<p><a href="https://tvm.apache.org/docs/reference/api/python/autotvm.html">https://tvm.apache.org/docs/reference/api/python/autotvm.html</a></p>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/apache/tvm" title="GitHub Repository: apache/tvm">apache/tvm</a></p>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
<h2 id="data-distribution-shifts-and-monitoring">Data distribution shifts and monitoring<a class="headerlink" href="#data-distribution-shifts-and-monitoring" title="Permanent link"></a></h2>
<h3 id="data-drift">Data drift<a class="headerlink" href="#data-drift" title="Permanent link"></a></h3>
<p>is a change in the statistical properties and characteristics of the input data. It occurs when a machine learning model is in production, as the data it encounters deviates from the data the model was initially trained on or earlier production data. </p>
<p>This shift in input data distribution can lead to a decline in the model&rsquo;s performance. The reason is, when you create a machine learning model, you can expect it to perform well on data similar to the data used to train it. However, it might struggle to make accurate predictions or decisions if the data keeps changing and the model cannot generalize beyond what it has seen in training.</p>
<p>In simple terms, data drift is a change in the model inputs the model is not trained to handle. Detecting and addressing data drift is vital to maintaining ML model reliability in dynamic settings.</p>
<p>How to <a href="https://madewithml.com/courses/mlops/monitoring/#drift">detect drift example</a>  </p>
<p><a href="https://www.evidentlyai.com/ml-in-production/data-drift">https://www.evidentlyai.com/ml-in-production/data-drift</a></p>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/SeldonIO/alibi-detect?tab=readme-ov-file" title="GitHub Repository: SeldonIO/alibi-detect?tab=readme-ov-file">SeldonIO/alibi-detect?tab=readme-ov-file</a></p>
<p><a href="https://madewithml.com/courses/mlops/monitoring/#drift">https://madewithml.com/courses/mlops/monitoring/#drift</a></p>
<h3 id="data-validation">Data validation<a class="headerlink" href="#data-validation" title="Permanent link"></a></h3>
<p>Data validation is a term used to describe the process of checking the accuracy and quality of source data to ensure accurate output. Implementing data validation processes for a ML model helps to mitigate “garbage in = garbage out” scenarios, where poor quality data produces a poorly functioning model.</p>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/great-expectations/great_expectations" title="GitHub Repository: great-expectations/great_expectations">great-expectations/great_expectations</a></p>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/awslabs/deequ" title="GitHub Repository: awslabs/deequ">awslabs/deequ</a></p>
<h2 id="continual-learning">Continual learning<a class="headerlink" href="#continual-learning" title="Permanent link"></a></h2>
<p>training paradigm where a model is updated with some amount of incomming samples or baches of samples(1+&hellip;512+&hellip;1024+....) with a periodicity (immediately, every minute, every 5 minutes, very hour, once month, once a year&hellip;). </p>
<p>The updated model shouldnt be deployed until its been evalueated. This means that you shouldnt make chanches to existing model directly. Instead, you create replica and update this replica with new data and only replace existing model with updated replica if updated replica proves to be better. The existing model is called the champion model, and the updated replica, the chalanger.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/continual_learning.jpg" height="500"></p>
<h3 id="stateless-retraining-vs-statefull-training">Stateless Retraining vs Statefull Training<a class="headerlink" href="#stateless-retraining-vs-statefull-training" title="Permanent link"></a></h3>
<p><strong>Stateless Retraining</strong> - the model is trained from scratch each time.</p>
<p><strong>Statefull Training</strong> - the model continues learnig on new data.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/stateless_vs_statfull.jpg" height="385"></p>
<h3 id="test-in-production">Test in production<a class="headerlink" href="#test-in-production" title="Permanent link"></a></h3>
<ul>
<li>
<p><strong>Shadow deployment</strong>: trafic routed to champion and chalenger, but only champion is serving clients</p>
</li>
<li>
<p><strong>A/B testing</strong>: purpose is usually to see users&rsquo; response (In a way, how much they like it). But you know that the new version works. So, you actually send randomly both versions of the application to all of them. It can be 50-50, 80-20, 90-10, anything. You might want to see which version attracts more clients and stuff like that.</p>
</li>
<li>
<p><strong>Canary release</strong> is more focused on how well works the new feature. Or if it actually works. It usually will be 90-10, 80-20, A &gt;&gt; B. Never 50-50, because if it goes wrong, you don&rsquo;t want half of your users to have a bad experience. So you are not positive if the new version is going to work as expected.</p>
</li>
<li>
<p><strong>Interleaving experiments</strong></p>
</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/interleaving_experiments_3.jpg">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/interleaving_experiments.jpg" height="480">
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/interleaving_experiments_2.jpg"></p>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
<p><a href="https://dl.acm.org/doi/fullHtml/10.1145/3543873.3587572">https://dl.acm.org/doi/fullHtml/10.1145/3543873.3587572</a></p>
<h2 id="testing">Testing<a class="headerlink" href="#testing" title="Permanent link"></a></h2>
<p><a href="https://madewithml.com/courses/mlops/testing/">https://madewithml.com/courses/mlops/testing/</a></p>
<h3 id="behavioral-testing">Behavioral testing<a class="headerlink" href="#behavioral-testing" title="Permanent link"></a></h3>
<p>Besides just looking at metrics, we also want to conduct some behavioral sanity tests. Behavioral testing is the process of testing input data and expected outputs while treating the model as a black box.</p>
<p>Break down behavioral testing into three types of tests:</p>
<ul>
<li><strong>invariance</strong>: Changes should not affect outputs</li>
<li><strong>directional</strong>: Change should affect outputs</li>
<li><strong>minimum functionality</strong>: Simple combination of inputs and expected outputs</li>
</ul>
<p><a href="https://madewithml.com/courses/mlops/evaluation/">https://madewithml.com/courses/mlops/evaluation/</a></p>
<h2 id="ml-infrastructure">ML infrastructure<a class="headerlink" href="#ml-infrastructure" title="Permanent link"></a></h2>
<p>Examples:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/docker_ml.jpg" height="500"></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/torchserve.gif"></p>
<h2 id="ml-lifecycle">ML lifecycle<a class="headerlink" href="#ml-lifecycle" title="Permanent link"></a></h2>
<p>Example:</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/ml_lifecycle.jpg"></p>
<h2 id="mlops">MLOps<a class="headerlink" href="#mlops" title="Permanent link"></a></h2>
<h3 id="infrastructure">Infrastructure<a class="headerlink" href="#infrastructure" title="Permanent link"></a></h3>
<p>Layers:</p>
<ul>
<li>
<p><strong>Storage and compute</strong>: the storage layer is where data is collected and stored. The compute layer provides compute to run your ML workloads such as training a model, computing features, generating features, etc.</p>
</li>
<li>
<p><strong>Resource management</strong>: tools to schedule and orchestrate your workloads to make the most out of your available resources. Examples: Airflow, Kubeflow, Metaflow.</p>
</li>
<li>
<p><strong>ML platform</strong>: tools for aid the development of ML applications such as mdel stores, feature stores, monitoring tools. Examples: SageMaker, MLflow</p>
</li>
<li>
<p><strong>Development environment</strong></p>
</li>
</ul>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/infrastructure_1.jpg"></p>
<h4 id="development-environment">Development environment<a class="headerlink" href="#development-environment" title="Permanent link"></a></h4>
<p><a href="https://github.com/nteract/papermill">Papermill</a> - is a tool for parameterizing, executing, and analyzing Jupyter Notebooks</p>
<p><a href="https://eugeneyan.com/writing/experimentation-workflow-with-jupyter-papermill-mlflow/">MLFlow + Papermill</a></p>
<p><a href="https://docs.ray.io/en/latest/index.html">Ray</a> - distributed(parallel) computing for training and tuning models, ML frameworks agnostic</p>
<h2 id="running-ml-on-edge-an-mobile-devices">Running ML on edge an mobile devices<a class="headerlink" href="#running-ml-on-edge-an-mobile-devices" title="Permanent link"></a></h2>
<p>On-device machine learning is a paradigm shift from traditional cloud-based ML. It allows:</p>
<ul>
<li>Direct Processing: ML tasks are performed directly on the device, such as smartphones or tablets.</li>
<li>Independence from the Cloud: It operates without the need for a constant connection to the cloud.</li>
<li>Local Data Processing: Data is processed locally, ensuring faster response times and immediate inference. Inference, in this context, refers to the process of applying a trained machine learning model to make predictions or draw conclusions from input data.</li>
</ul>
<p>When planning the implementation on mobile devices, there are several considerations to keep in mind:</p>
<ul>
<li>Data Pre and Post-processing: On the web, we often perform data pre and post-processing using libraries like OpenCV and NumPy. However, these libraries may not be directly available in the mobile framework, so alternative approaches or libraries may be needed.</li>
<li>Choice of Language: The programming language used for web scripts becomes important, as not all web libraries and frameworks are easily transferrable to mobile platforms.</li>
<li>Accuracy vs. Constraints: It’s essential to define the desired accuracy for your mobile model and consider any time constraints. Smaller models are often preferred on mobile devices due to factors like app size and processing speed, but this may result in a drop in accuracy.</li>
<li>Memory Management: Unlike web devices, mobile devices have more limited memory resources. Larger model sizes and complex computational code can lead to memory-related issues and even cause the mobile app to crash.</li>
</ul>
<p>There are various tools and frameworks that can help us add on-device ML. </p>
<p>For Android app:</p>
<ul>
<li>ML Kit: A Google SDK that provides ready-to-use ML models and APIs for common tasks such as barcode scanning, text recognition, face detection, pose detection, etc. The ML Kit also supports custom TensorFlow Lite models and AutoML Vision Edge models.</li>
<li>MediaPipe: A Google framework that enables building custom ML pipelines using pre-built components such as graphs, calculators, models, etc. MediaPipe also provides ready-to-use solutions for common tasks such as face mesh detection, hand tracking, object detection, etc.</li>
<li>TFLite: A Google library that allows running TensorFlow models on mobile devices with low latency and a small binary size. TFLite also supports hardware acceleration, model optimization, and metadata extraction.</li>
</ul>
<p>For IOS devices:</p>
<ul>
<li>Create ML</li>
<li>Core ML</li>
</ul>
<p><a href="https://gursimarsm.medium.com/how-to-add-machine-learning-to-an-android-app-5bb8c6c185ef">https://gursimarsm.medium.com/how-to-add-machine-learning-to-an-android-app-5bb8c6c185ef</a></p>
<p><a href="https://medium.com/eumentis/how-we-navigated-running-machine-learning-models-on-edge-device-7c67b1ff1b97">https://medium.com/eumentis/how-we-navigated-running-machine-learning-models-on-edge-device-7c67b1ff1b97</a></p>
<p><strong>Amazon SageMaker Neo</strong>  </p>
<p>provides compilation support for popular machine learning frameworks. You can deploy your Neo-compiled edge devices such as the Raspberry Pi 3, Texas Instruments&rsquo; Sitara, Jetson TX1, and more.</p>
<p>Automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. You start with a machine learning model already built with DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost and trained in Amazon SageMaker or anywhere else. Then you choose your target hardware platform, which can be a SageMaker hosting instance or an edge device based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, or Texas Instruments. With a single click, SageMaker Neo optimizes the trained model and compiles it into an executable. </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/sage_maker_neo.jpg"></p>
<p><a href="https://aws.amazon.com/sagemaker/neo/">https://aws.amazon.com/sagemaker/neo/</a></p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo-edge-devices.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo-edge-devices.html</a></p>
<h2 id="running-ml-in-browsers">Running ML in browsers<a class="headerlink" href="#running-ml-in-browsers" title="Permanent link"></a></h2>
<h3 id="compiling-to-js">Compiling to JS<a class="headerlink" href="#compiling-to-js" title="Permanent link"></a></h3>
<ul>
<li><a href="https://www.tensorflow.org/js">TensorFlow.js</a></li>
<li><a href="https://github.com/BrainJS/brain.js">brain.js</a></li>
<li><a href="https://github.com/cazala/synaptic">Synaptic</a></li>
</ul>
<h3 id="compiling-to-wasm">Compiling to WASM<a class="headerlink" href="#compiling-to-wasm" title="Permanent link"></a></h3>
<p>After model is built in scikit-learn, PyTorch, TensorFlow, or whatever other framework you can compile model to WASM. You get back executable file that you can just use with javascript.</p>
<h2 id="frameworks">Frameworks<a class="headerlink" href="#frameworks" title="Permanent link"></a></h2>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/frameworks_comparison.jpg"></p>
<p><strong>pytorch</strong> vs <strong>tensorflow</strong> <a href="https://www.upwork.com/resources/pytorch-vs-tensorflow">https://www.upwork.com/resources/pytorch-vs-tensorflow</a></p>
<p><a href="https://mljourney.com/comparison-of-popular-machine-learning-frameworks/">https://mljourney.com/comparison-of-popular-machine-learning-frameworks/</a></p>
<p><a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a></p>
<h2 id="python-ecosystem">Python ecosystem<a class="headerlink" href="#python-ecosystem" title="Permanent link"></a></h2>
<p><strong><a href="https://numpy.org/">NumPy</a></strong> -  a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</p>
<p><strong><a href="https://matplotlib.org/">Matplotlib</a></strong> is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible. </p>
<p><strong><a href="https://seaborn.pydata.org/index.html">seaborn</a></strong> is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>
<p><strong><a href="https://pandas.pydata.org/">Pandas</a></strong> is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool</p>
<p><strong><a href="https://scikit-learn.org/">scikit-learn</a></strong> - Simple and efficient tools for predictive data analysis. Collection of algorithms. Open-source machine learning library. It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Collection of toy datasets generation methods.</p>
<p><strong><a href="https://lightning.ai/docs/torchmetrics/stable/">TorchMetricks</a></strong>  is a collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. </p>
<p><strong><a href="https://github.com/TylerYep/torchinfo">torchinfo</a></strong> provides information complementary to what is provided by <code>print(your_model)</code> in PyTorch. It prints topology of model, summary, etc </p>
<p><strong><a href="https://www.tensorflow.org/tensorboard">TensorBoard</a></strong> provides the visualization and tooling needed for machine learning experimentation. Experiment tracking, performance plots, etc. </p>
<p><strong><a href="https://www.gradio.app/">Gradio</a></strong> -  is the fastest way to demo your machine learning model with a friendly web interface</p>
<p><strong><a href="https://github.com/SeldonIO/alibi-detect?tab=readme-ov-file">alibi-detect</a></strong> - outlier, adversarial and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. Both TensorFlow and PyTorch backends are supported for drift detection.</p>
<p><strong><a href="https://github.com/great-expectations/great_expectations">great_expectations</a></strong> - data validation</p>
<p><strong><a href="https://docs.ray.io/en/latest/index.html">Ray</a></strong> - distributed(parallel) computing for training and tuning models, ML frameworks agnostic</p>
<h2 id="scikit-learn">Scikit-learn<a class="headerlink" href="#scikit-learn" title="Permanent link"></a></h2>
<h3 id="traintest-split">Train/Test split<a class="headerlink" href="#traintest-split" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;Label&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Label&#39;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>
</pre></div>

<h3 id="scaler">Scaler<a class="headerlink" href="#scaler" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

<h3 id="grid-search_1">Grid search<a class="headerlink" href="#grid-search_1" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="n">base_elastic_model</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">()</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;alpha&#39;</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span>
              <span class="s">&#39;l1_ratio&#39;</span><span class="p">:[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># verbose number a personal preference</span>
<span class="n">grid_model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">base_elastic_model</span><span class="p">,</span>
                          <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                          <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
                          <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="c1"># ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=1,</span>
<span class="c1">#            max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="c1">#            random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>

<span class="n">grid_model</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># {&#39;alpha&#39;: 0.1, &#39;l1_ratio&#39;: 1}</span>
</pre></div>

<h3 id="performance-metric">Performance metric<a class="headerlink" href="#performance-metric" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="p">,</span><span class="n">accuracy_score</span>

<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">pred</span><span class="p">))</span>
<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">pred</span><span class="p">)</span>
</pre></div>

<h3 id="pipeline">Pipeline<a class="headerlink" href="#pipeline" title="Permanent link"></a></h3>
<p>If your parameter grid is going inside a PipeLine, your parameter name needs to be specified in the following manner:</p>
<pre><code>chosen_string_name + two underscores + parameter key name
model_name + __ + parameter name
knn_model + __ + n_neighbors
knn_model__n_neighbors
</code></pre>
<p>The reason we have to do this is because it let&rsquo;s scikit-learn know what operation in the pipeline these parameters are related to (otherwise it might think n_neighbors was a parameter in the scaler).</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="n">operations</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),(</span><span class="s">&#39;knn&#39;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)]</span>

<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">operations</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;knn__n_neighbors&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]}</span>

<span class="n">full_cv_classifier</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span><span class="n">param_grid</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">full_cv_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cv</span><span class="p">,</span><span class="n">y_cv</span><span class="p">)</span>
<span class="n">full_cv_classifier</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">full_cv_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

<h3 id="exportimport-model">Export/Import model<a class="headerlink" href="#exportimport-model" title="Permanent link"></a></h3>
<p>Its possible import/export file of model, scaller, pipeline
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span><span class="p">,</span> <span class="n">load</span>

<span class="n">dump</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="s">&#39;sales_model.joblib&#39;</span><span class="p">)</span> 

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s">&#39;sales_model.joblib&#39;</span><span class="p">)</span>
</pre></div></p>
<h3 id="encode-labelstargets">Encode labels(targets)<a class="headerlink" href="#encode-labelstargets" title="Permanent link"></a></h3>
<p>do not encode features!
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">])</span> <span class="c1"># array([2, 2, 1]...)</span>

<span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span> <span class="c1"># [&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div></p>
<h3 id="encode-categorical-features">Encode categorical features<a class="headerlink" href="#encode-categorical-features" title="Permanent link"></a></h3>
<p>OneHot:
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="s">&#39;type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;type&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s">&#39;type&#39;</span><span class="p">,</span><span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># dummy_na=True for nan value category </span>

<span class="c1"># OR</span>

<span class="n">type_one_hot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;type&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s">&#39;type&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">type_one_hot</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div></p>
<h3 id="manual-map-categories-to-int">Manual map categories to int<a class="headerlink" href="#manual-map-categories-to-int" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s">&#39;male&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;female&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
</pre></div>

<h3 id="feature-importance-for-model">Feature importance for model<a class="headerlink" href="#feature-importance-for-model" title="Permanent link"></a></h3>
<div class="highlight"><pre><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> 
             <span class="n">data</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> 
             <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;Feature Importance&#39;</span><span class="p">])</span>
</pre></div>

<h3 id="ensemble-voting-classifier">Ensemble. Voting classifier<a class="headerlink" href="#ensemble-voting-classifier" title="Permanent link"></a></h3>
<p><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
    <span class="n">voting</span><span class="o">=</span><span class="s">&#39;hard&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s">&#39;naive Bayes&#39;</span><span class="p">,</span> <span class="s">&#39;Ensemble&#39;</span><span class="p">]):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s"> (+/- </span><span class="si">%0.2f</span><span class="s">) [</span><span class="si">%s</span><span class="s">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>

<span class="c1"># Accuracy: 0.95 (+/- 0.04) [Logistic Regression]</span>
<span class="c1"># Accuracy: 0.94 (+/- 0.04) [Random Forest]</span>
<span class="c1"># Accuracy: 0.91 (+/- 0.04) [naive Bayes]</span>
<span class="c1"># Accuracy: 0.95 (+/- 0.04) [Ensemble]</span>
</pre></div>
<a href="https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier">https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier</a></p>
<h3 id="regular-vs-cvcross-validation-model">regular vs CV(cross validation) model<a class="headerlink" href="#regular-vs-cvcross-validation-model" title="Permanent link"></a></h3>
<h2 id="plots">Plots<a class="headerlink" href="#plots" title="Permanent link"></a></h2>
<p><div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">)</span>
</pre></div>
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/catplot.png"></p>
<hr />
<p><div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">&quot;box&quot;</span><span class="p">)</span>
</pre></div>
min, max, median, outliers</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/catplot2.png"></p>
<hr />
<p><div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;class&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">&quot;sex&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">&quot;boxen&quot;</span><span class="p">)</span>
</pre></div>
<img src="/home/isidzukuri/Desktop/ml_algorithms/images/catplot3.png"></p>
<hr />
<p>more catplots <a href="https://seaborn.pydata.org/generated/seaborn.catplot.html">https://seaborn.pydata.org/generated/seaborn.catplot.html</a></p>
<hr />
<p><div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">&quot;species&quot;</span><span class="p">)</span>
</pre></div>
displays the relationships between columns </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/pairplot.png"></p>
<hr />
<p>more pairplots <a href="https://seaborn.pydata.org/generated/seaborn.pairplot.html">https://seaborn.pydata.org/generated/seaborn.pairplot.html</a></p>
<hr />
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span>
</pre></div>

<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/tree_plot.jpg"></p>
<hr />
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tips</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;total_bill&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;tip&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">&quot;time&quot;</span><span class="p">)</span>
</pre></div>

<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/scatterplot_5_0.png"></p>
<p><a href="https://seaborn.pydata.org/generated/seaborn.scatterplot.html">https://seaborn.pydata.org/generated/seaborn.scatterplot.html</a></p>
<hr />
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;island&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;body_mass_g&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">&quot;sex&quot;</span><span class="p">)</span>
</pre></div>

<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/barplot_9_0.png"></p>
<p><a href="https://seaborn.pydata.org/generated/seaborn.barplot.html">https://seaborn.pydata.org/generated/seaborn.barplot.html</a></p>
<hr />
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">penguins</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;flipper_length_mm&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>

<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/histplot_7_0.png"></p>
<p><a href="https://seaborn.pydata.org/generated/seaborn.histplot.html">https://seaborn.pydata.org/generated/seaborn.histplot.html</a></p>
<hr />
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">flights</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">&quot;year&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">&quot;passengers&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">&quot;month&quot;</span><span class="p">)</span>
</pre></div>

<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/lineplot_13_0.png"></p>
<p><a href="https://seaborn.pydata.org/generated/seaborn.lineplot.html">https://seaborn.pydata.org/generated/seaborn.lineplot.html</a></p>
<hr />
<h2 id="pandas-snippets">Pandas snippets<a class="headerlink" href="#pandas-snippets" title="Permanent link"></a></h2>
<p>read csv
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;../DATA/bank-full.csv&quot;</span><span class="p">)</span>
</pre></div></p>
<p>csv write
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">&#39;new_file.csv&#39;</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div></p>
<p>discover data frame:
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div></p>
<p>get unique values of column:
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="s">&#39;Contract&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div></p>
<p>rows count
<div class="highlight"><pre><span class="nb">len</span><span class="p">(</span><span class="n">hotels</span><span class="p">)</span>
</pre></div></p>
<p>count of missing values per column:
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div></p>
<p>select where column is null:
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Agriculture&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()][</span><span class="s">&#39;Country&#39;</span><span class="p">]</span>
</pre></div></p>
<p>fill null with values:
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Agriculture&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Agriculture&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div></p>
<p>fill null with mean of column:
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="s">&#39;Climate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Climate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">&#39;Region&#39;</span><span class="p">)[</span><span class="s">&#39;Climate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s">&#39;mean&#39;</span><span class="p">))</span>
</pre></div></p>
<p>drop rows where is null:
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
</pre></div></p>
<p>drop column
<div class="highlight"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&quot;Country&quot;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div></p>
<p>sort values:
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">&#39;adr&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div></p>
<p>correlation of column to others
<div class="highlight"><pre><span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">()[</span><span class="s">&#39;Some column name&#39;</span><span class="p">]</span>
</pre></div></p>
<p>filter
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;total_of_special_requests&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span>

<span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;total_bill&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s">&#39;Male&#39;</span><span class="p">)]</span>

<span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;total_bill&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;sex&#39;</span><span class="p">]</span><span class="o">!=</span><span class="s">&#39;Male&#39;</span><span class="p">)]</span>

<span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;day&#39;</span><span class="p">]</span> <span class="o">==</span><span class="s">&#39;Sun&#39;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;day&#39;</span><span class="p">]</span><span class="o">==</span><span class="s">&#39;Sat&#39;</span><span class="p">)]</span> <span class="c1"># or</span>

<span class="n">options</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Sat&#39;</span><span class="p">,</span><span class="s">&#39;Sun&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s">&#39;day&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
</pre></div></p>
<p>transform
<div class="highlight"><pre><span class="n">df</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div></p>
<p>reshape 3d to 2d:
<div class="highlight"><pre><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="n">image_as_array</span><span class="o">.</span><span class="n">shape</span>
<span class="n">image_as_array2d</span> <span class="o">=</span> <span class="n">image_as_array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
</pre></div></p>
<p><strong>cheat sheets</strong>:</p>
<ul>
<li>
<p><a href="https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf">https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf</a></p>
</li>
<li>
<p><a href="https://medium.com/bitgrit-data-science-publication/40-useful-pandas-snippets-d7833472d12f">https://medium.com/bitgrit-data-science-publication/40-useful-pandas-snippets-d7833472d12f</a></p>
</li>
<li>
<p><a href="https://gist.github.com/bsweger/e5817488d161f37dcbd2">https://gist.github.com/bsweger/e5817488d161f37dcbd2</a></p>
</li>
</ul>
<h2 id="glossary">Glossary<a class="headerlink" href="#glossary" title="Permanent link"></a></h2>
<p><strong>Embedding</strong> - learnable representation (start with random numbers and improve over time)</p>
<p><strong>Multilayer perceptron</strong> -  (MLP) is a name for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[1]</p>
<p>Modern feedforward networks are trained using the backpropagation method[2][3][4][5][6] and are colloquially referred to as the &ldquo;vanilla&rdquo; neural networks.[7]</p>
<p>MLPs grew out of an effort to improve single-layer perceptrons, which could only distinguish linearly separable data. A perceptron traditionally used a Heaviside step function as its nonlinear activation function. However, the backpropagation algorithm requires that modern MLPs use continuous activation functions such as sigmoid or ReLU.</p>
<p><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">https://en.wikipedia.org/wiki/Multilayer_perceptron</a></p>
<p><strong>Residual Connection</strong> - are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. </p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/residual_connection.png"  height="200"></p>
<p><strong>Learning rate warmup</strong> - start with small learning rate and warmup to sertain value.<a href="https://paperswithcode.com/method/linear-warmup">https://paperswithcode.com/method/linear-warmup</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/lr_warmup.png" height="250"></p>
<p><strong>Learning rate decay</strong> - turn down lr closer to better results</p>
<p><strong>Gragient clipping</strong> - cut some gradients <a href="https://paperswithcode.com/method/gradient-clipping">https://paperswithcode.com/method/gradient-clipping</a></p>
<p><strong>Multi-head Attention</strong> is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). <a href="https://paperswithcode.com/method/multi-head-attention">https://paperswithcode.com/method/multi-head-attention</a></p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/multi-head-attention.png" height="250"></p>
<p><strong>Cardinality</strong> is a mathematics term that refers to the number of unique elements in a set. It can be categorized into high and low cardinality. High cardinality is present when a column or row in a database has many distinct values, whereas low cardinality involves many repeated values.</p>
<p><img src="/home/isidzukuri/Desktop/ml_algorithms/images/cardinality.png" height="350"></p>
<p><a href="https://www.splunk.com/en_us/blog/learn/cardinality-metrics-monitoring-observability.html">https://www.splunk.com/en_us/blog/learn/cardinality-metrics-monitoring-observability.html</a></p>
<h2 id="math-symbols">Math symbols<a class="headerlink" href="#math-symbols" title="Permanent link"></a></h2>
<p>∈ - elements</p>
<p>R - real numbers</p>
<h2 id="useful-links">Useful links<a class="headerlink" href="#useful-links" title="Permanent link"></a></h2>
<p>Machine Learning Glossary:</p>
<ul>
<li><a href="https://developers.google.com/machine-learning/glossary">https://developers.google.com/machine-learning/glossary</a></li>
</ul>
<p>Datasets, models:</p>
<ul>
<li>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/huggingface/pytorch-image-models" title="GitHub Repository: huggingface/pytorch-image-models">huggingface/pytorch-image-models</a></p>
</li>
<li>
<p><a href="https://huggingface.co/">https://huggingface.co/</a></p>
</li>
<li>
<p><a href="https://www.kaggle.com/">https://www.kaggle.com/</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/sota">https://paperswithcode.com/sota</a></p>
</li>
<li>
<p><a class="magiclink magiclink-github magiclink-mention" href="https://github.com/satellite-image-deep-learning" title="GitHub User: satellite-image-deep-learning">@satellite-image-deep-learning</a> [topological, satelite images]</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/MNIST_database">https://en.wikipedia.org/wiki/MNIST_database</a> [images]</p>
</li>
<li>
<p><a class="magiclink magiclink-github magiclink-repository" href="https://github.com/zalandoresearch/fashion-mnist" title="GitHub Repository: zalandoresearch/fashion-mnist">zalandoresearch/fashion-mnist</a> [images]</p>
</li>
<li>
<p><a href="https://www.image-net.org/">https://www.image-net.org/</a> [images]</p>
</li>
<li>
<p><a href="https://github.com/lucidrains?tab=repositories">https://github.com/lucidrains?tab=repositories</a></p>
</li>
</ul>
<p>Courses:</p>
<ul>
<li>
<p><a href="https://www.udemy.com/course/aws-machine-learning/">https://www.udemy.com/course/aws-machine-learning/</a></p>
</li>
<li>
<p><a href="https://www.udemy.com/course/pytorch-for-deep-learning/">https://www.udemy.com/course/pytorch-for-deep-learning/</a></p>
</li>
<li>
<p><a href="https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/">https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/</a></p>
</li>
<li>
<p><a href="https://www.learnpytorch.io/">https://www.learnpytorch.io/</a></p>
</li>
</ul>
<p>Lectures:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=1">https://www.youtube.com/watch?v=ErnWZxJovaM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=1</a></li>
</ul>
<p>Articles:</p>
<ul>
<li>
<p><a href="https://arxiv.org/">https://arxiv.org/</a></p>
</li>
<li>
<p><a href="https://eugeneyan.com/">https://eugeneyan.com/</a></p>
</li>
<li>
<p><a href="https://jalammar.github.io/">https://jalammar.github.io/</a></p>
</li>
</ul>
<p>Books:</p>
<ul>
<li>
<p><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">&ldquo;Designing Machine Learning Systems&rdquo; by Chip Huyen</a></p>
</li>
<li>
<p><a href="https://huyenchip.com/ml-interviews-book/contents/4.3.1-courses.html">&ldquo;Introduction to Machine Learning Interviews&rdquo; by Chip Huyen</a></p>
</li>
</ul>
<p>Cheatsheets:</p>
<ul>
<li>
<p><a href="/home/isidzukuri/Desktop/ml_algorithms/pdf/ML%2BCheat%2BSheet_2.pdf">ML model selection #1</a></p>
</li>
<li>
<p><a href="/home/isidzukuri/Desktop/ml_algorithms/pdf/Model_Selection_in_Machine_Learning_for_Different_Models.avif">ML model selection #2</a></p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">ML model selection #3</a></p>
</li>
</ul>
<p>Online NN builder:</p>
<ul>
<li>
<p><a href="https://playground.tensorflow.org/">https://playground.tensorflow.org/</a></p>
</li>
<li>
<p><a href="https://poloclub.github.io/transformer-explainer/">https://poloclub.github.io/transformer-explainer/</a></p>
</li>
</ul>
<p>Performance:</p>
<ul>
<li><a href="https://horace.io/brrr_intro.html">https://horace.io/brrr_intro.html</a></li>
</ul>
<p>Social media:</p>
<ul>
<li>
<p><a href="https://x.com/_akhaliq">https://x.com/_akhaliq</a></p>
</li>
<li>
<p><a href="https://x.com/NeurIPSConf">https://x.com/NeurIPSConf</a></p>
</li>
<li>
<p><a href="https://x.com/iclr_conf">https://x.com/iclr_conf</a></p>
</li>
<li>
<p><a href="https://x.com/icmlconf">https://x.com/icmlconf</a></p>
</li>
</ul></article></body></html>